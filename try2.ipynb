{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sample_weight(labels, num_class, use_sample_weight=True):\n",
    "    if not use_sample_weight:\n",
    "        return np.ones(len(labels)) / len(labels)\n",
    "    count = np.zeros(num_class)\n",
    "    for i in range(num_class):\n",
    "        count[i] = np.sum(labels==i)\n",
    "    sample_weight = np.zeros(labels.shape)\n",
    "    for i in range(num_class):\n",
    "        sample_weight[np.where(labels==i)[0]] = count[i]/np.sum(count)\n",
    "    \n",
    "    return sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_tensor(y, num_dim):\n",
    "    y_onehot = torch.zeros(y.shape[0], num_dim)\n",
    "    y_onehot.scatter_(1, y.view(-1,1), 1)\n",
    "    \n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance_torch(x1, x2=None, eps=1e-8):\n",
    "    x2 = x1 if x2 is None else x2\n",
    "    w1 = x1.norm(p=2, dim=1, keepdim=True)\n",
    "    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n",
    "    return 1 - torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse(x):\n",
    "    x_typename = torch.typename(x).split('.')[-1]\n",
    "    sparse_tensortype = getattr(torch.sparse, x_typename)\n",
    "    indices = torch.nonzero(x)\n",
    "    if len(indices.shape) == 0:  # if all elements are zeros\n",
    "        return sparse_tensortype(*x.shape)\n",
    "    indices = indices.t()\n",
    "    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "    return sparse_tensortype(indices, values, x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_adj_mat_parameter(edge_per_node, data, metric=\"cosine\"):\n",
    "    assert metric == \"cosine\", \"Only cosine distance implemented\"\n",
    "    dist = cosine_distance_torch(data, data)\n",
    "    parameter = torch.sort(dist.reshape(-1,)).values[edge_per_node*data.shape[0]]\n",
    "    #return np.asscalar(parameter.data.cpu().numpy())\n",
    "    return parameter.data.cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_from_dist_tensor(dist, parameter, self_dist=True):\n",
    "    if self_dist:\n",
    "        assert dist.shape[0]==dist.shape[1], \"Input is not pairwise dist matrix\"\n",
    "    g = (dist <= parameter).float()\n",
    "    if self_dist:\n",
    "        diag_idx = np.diag_indices(g.shape[0])\n",
    "        g[diag_idx[0], diag_idx[1]] = 0\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adj_mat_tensor(data, parameter, metric=\"cosine\"):\n",
    "    assert metric == \"cosine\", \"Only cosine distance implemented\"\n",
    "    dist = cosine_distance_torch(data, data)\n",
    "    g = graph_from_dist_tensor(dist, parameter, self_dist=True)\n",
    "    if metric == \"cosine\":\n",
    "        adj = 1-dist\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    adj = adj*g \n",
    "    adj_T = adj.transpose(0,1)\n",
    "    I = torch.eye(adj.shape[0])\n",
    "    if cuda:\n",
    "        I = I.cuda()\n",
    "    adj = adj + adj_T*(adj_T > adj).float() - adj*(adj_T > adj).float()\n",
    "    adj = F.normalize(adj + I, p=1)\n",
    "    adj = to_sparse(adj)\n",
    "    \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_adj_mat_tensor(data, trte_idx, parameter, metric=\"cosine\"):\n",
    "    assert metric == \"cosine\", \"Only cosine distance implemented\"\n",
    "    adj = torch.zeros((data.shape[0], data.shape[0]))\n",
    "    if cuda:\n",
    "        adj = adj.cuda()\n",
    "    num_tr = len(trte_idx[\"tr\"])\n",
    "    \n",
    "    dist_tr2te = cosine_distance_torch(data[trte_idx[\"tr\"]], data[trte_idx[\"te\"]])\n",
    "    g_tr2te = graph_from_dist_tensor(dist_tr2te, parameter, self_dist=False)\n",
    "    if metric == \"cosine\":\n",
    "        adj[:num_tr,num_tr:] = 1-dist_tr2te\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    adj[:num_tr,num_tr:] = adj[:num_tr,num_tr:]*g_tr2te\n",
    "    \n",
    "    dist_te2tr = cosine_distance_torch(data[trte_idx[\"te\"]], data[trte_idx[\"tr\"]])\n",
    "    g_te2tr = graph_from_dist_tensor(dist_te2tr, parameter, self_dist=False)\n",
    "    if metric == \"cosine\":\n",
    "        adj[num_tr:,:num_tr] = 1-dist_te2tr\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    adj[num_tr:,:num_tr] = adj[num_tr:,:num_tr]*g_te2tr # retain selected edges\n",
    "    \n",
    "    adj_T = adj.transpose(0,1)\n",
    "    I = torch.eye(adj.shape[0])\n",
    "    if cuda:\n",
    "        I = I.cuda()\n",
    "    adj = adj + adj_T*(adj_T > adj).float() - adj*(adj_T > adj).float()\n",
    "    adj = F.normalize(adj + I, p=1)\n",
    "    adj = to_sparse(adj)\n",
    "    \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "           m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        nn.init.xavier_normal_(self.weight.data)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.sparse.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_E(nn.Module):\n",
    "    def __init__(self, in_dim, hgcn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.gc1 = GraphConvolution(in_dim, hgcn_dim[0])\n",
    "        self.gc2 = GraphConvolution(hgcn_dim[0], hgcn_dim[1])\n",
    "        self.gc3 = GraphConvolution(hgcn_dim[1], hgcn_dim[2])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.leaky_relu(x, 0.25)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        x = F.leaky_relu(x, 0.25)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc3(x, adj)\n",
    "        x = F.leaky_relu(x, 0.25)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_1(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.clf = nn.Sequential(nn.Linear(in_dim, out_dim))\n",
    "        self.clf.apply(xavier_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.clf(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCDN(nn.Module):\n",
    "    def __init__(self, num_view, num_cls, hvcdn_dim):\n",
    "        super().__init__()\n",
    "        self.num_cls = num_cls\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(pow(num_cls, num_view), hvcdn_dim),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Linear(hvcdn_dim, num_cls)\n",
    "        )\n",
    "        self.model.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, in_list):\n",
    "        num_view = len(in_list)\n",
    "        for i in range(num_view):\n",
    "            in_list[i] = torch.sigmoid(in_list[i])\n",
    "        x = torch.reshape(torch.matmul(in_list[0].unsqueeze(-1), in_list[1].unsqueeze(1)),(-1,pow(self.num_cls,2),1))\n",
    "        for i in range(2,num_view):\n",
    "            x = torch.reshape(torch.matmul(x, in_list[i].unsqueeze(1)),(-1,pow(self.num_cls,i+1),1))\n",
    "        vcdn_feat = torch.reshape(x, (-1,pow(self.num_cls,num_view)))\n",
    "        output = self.model(vcdn_feat)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_dict(num_view, num_class, dim_list, dim_he_list, dim_hc, gcn_dopout=0.5):\n",
    "    model_dict = {}\n",
    "    for i in range(num_view):\n",
    "        model_dict[\"E{:}\".format(i+1)] = GCN_E(dim_list[i], dim_he_list, gcn_dopout)\n",
    "        model_dict[\"C{:}\".format(i+1)] = Classifier_1(dim_he_list[-1], num_class)\n",
    "    if num_view >= 2:\n",
    "        model_dict[\"C\"] = VCDN(num_view, num_class, dim_hc)\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def init_optim(num_view, model_dict, lr_e=1e-4, lr_c=1e-4):\n",
    "    optim_dict = {}\n",
    "    for i in range(num_view):\n",
    "        optim_dict[\"C{:}\".format(i+1)] = torch.optim.Adam(\n",
    "                list(model_dict[\"E{:}\".format(i+1)].parameters())+list(model_dict[\"C{:}\".format(i+1)].parameters()), \n",
    "                lr=lr_e)\n",
    "    if num_view >= 2:\n",
    "        optim_dict[\"C\"] = torch.optim.Adam(model_dict[\"C\"].parameters(), lr=lr_c)\n",
    "    return optim_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_trte_data(data_folder, view_list):\n",
    "    num_view = len(view_list)\n",
    "    labels_tr = np.loadtxt(os.path.join(data_folder, \"labels_tr.csv\"), delimiter=',')\n",
    "    labels_te = np.loadtxt(os.path.join(data_folder, \"labels_te.csv\"), delimiter=',')\n",
    "    labels_tr = labels_tr.astype(int)\n",
    "    labels_te = labels_te.astype(int)\n",
    "    data_tr_list = []\n",
    "    data_te_list = []\n",
    "    for i in view_list:\n",
    "        data_tr_list.append(np.loadtxt(os.path.join(data_folder, str(i)+\"_tr.csv\"), delimiter=','))\n",
    "        data_te_list.append(np.loadtxt(os.path.join(data_folder, str(i)+\"_te.csv\"), delimiter=','))\n",
    "    num_tr = data_tr_list[0].shape[0]\n",
    "    num_te = data_te_list[0].shape[0]\n",
    "    data_mat_list = []\n",
    "    for i in range(num_view):\n",
    "        data_mat_list.append(np.concatenate((data_tr_list[i], data_te_list[i]), axis=0))\n",
    "    data_tensor_list = []\n",
    "    for i in range(len(data_mat_list)):\n",
    "        data_tensor_list.append(torch.FloatTensor(data_mat_list[i]))\n",
    "        if cuda:\n",
    "            data_tensor_list[i] = data_tensor_list[i].cuda()\n",
    "    idx_dict = {}\n",
    "    idx_dict[\"tr\"] = list(range(num_tr))\n",
    "    idx_dict[\"te\"] = list(range(num_tr, (num_tr+num_te)))\n",
    "    data_train_list = []\n",
    "    data_all_list = []\n",
    "    for i in range(len(data_tensor_list)):\n",
    "        data_train_list.append(data_tensor_list[i][idx_dict[\"tr\"]].clone())\n",
    "        data_all_list.append(torch.cat((data_tensor_list[i][idx_dict[\"tr\"]].clone(),\n",
    "                                       data_tensor_list[i][idx_dict[\"te\"]].clone()),0))\n",
    "    labels = np.concatenate((labels_tr, labels_te))\n",
    "    \n",
    "    return data_train_list, data_all_list, idx_dict, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter):\n",
    "    adj_metric = \"cosine\" # cosine distance\n",
    "    adj_train_list = []\n",
    "    adj_test_list = []\n",
    "    for i in range(len(data_tr_list)):\n",
    "        adj_parameter_adaptive = cal_adj_mat_parameter(adj_parameter, data_tr_list[i], adj_metric)\n",
    "        adj_train_list.append(gen_adj_mat_tensor(data_tr_list[i], adj_parameter_adaptive, adj_metric))\n",
    "        adj_test_list.append(gen_test_adj_mat_tensor(data_trte_list[i], trte_idx, adj_parameter_adaptive, adj_metric))\n",
    "    \n",
    "    return adj_train_list, adj_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_list, adj_list, label, one_hot_label, sample_weight, model_dict, optim_dict, train_VCDN=True):\n",
    "    loss_dict = {}\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    for m in model_dict:\n",
    "        model_dict[m].train()    \n",
    "    num_view = len(data_list)\n",
    "    for i in range(num_view):\n",
    "        optim_dict[\"C{:}\".format(i+1)].zero_grad()\n",
    "        ci_loss = 0\n",
    "        ci = model_dict[\"C{:}\".format(i+1)](model_dict[\"E{:}\".format(i+1)](data_list[i],adj_list[i]))\n",
    "        ci_loss = torch.mean(torch.mul(criterion(ci, label),sample_weight))\n",
    "        ci_loss.backward()\n",
    "        optim_dict[\"C{:}\".format(i+1)].step()\n",
    "        loss_dict[\"C{:}\".format(i+1)] = ci_loss.detach().cpu().numpy().item()\n",
    "    if train_VCDN and num_view >= 2:\n",
    "        optim_dict[\"C\"].zero_grad()\n",
    "        c_loss = 0\n",
    "        ci_list = []\n",
    "        for i in range(num_view):\n",
    "            ci_list.append(model_dict[\"C{:}\".format(i+1)](model_dict[\"E{:}\".format(i+1)](data_list[i],adj_list[i])))\n",
    "        c = model_dict[\"C\"](ci_list)    \n",
    "        c_loss = torch.mean(torch.mul(criterion(c, label),sample_weight))\n",
    "        c_loss.backward()\n",
    "        optim_dict[\"C\"].step()\n",
    "        loss_dict[\"C\"] = c_loss.detach().cpu().numpy().item()\n",
    "    \n",
    "    return loss_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(data_list, adj_list, te_idx, model_dict):\n",
    "    for m in model_dict:\n",
    "        model_dict[m].eval()\n",
    "    num_view = len(data_list)\n",
    "    ci_list = []\n",
    "    for i in range(num_view):\n",
    "        ci_list.append(model_dict[\"C{:}\".format(i+1)](model_dict[\"E{:}\".format(i+1)](data_list[i],adj_list[i])))\n",
    "    if num_view >= 2:\n",
    "        c = model_dict[\"C\"](ci_list)    \n",
    "    else:\n",
    "        c = ci_list[0]\n",
    "    c = c[te_idx,:]\n",
    "    prob = F.softmax(c, dim=1).data.cpu().numpy()\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Predictions: [[91.32968  0.       0.       0.     ]\n",
      " [86.95827  0.       0.       0.     ]\n",
      " [85.60235  0.       0.       0.     ]\n",
      " ...\n",
      " [99.23234  0.       0.       0.     ]\n",
      " [99.48266  0.       0.       0.     ]\n",
      " [98.05835  0.       0.       0.     ]]\n",
      "Cross-Omics Discovery Tensor: [[ -664432.44 -1241418.5 ]\n",
      " [ -638173.1  -1192355.2 ]\n",
      " [ -643909.2  -1203072.5 ]\n",
      " [ -636319.   -1188890.8 ]\n",
      " [ -733917.25 -1371243.2 ]\n",
      " [ -664191.56 -1240968.2 ]\n",
      " [ -717302.25 -1340199.9 ]\n",
      " [ -710507.3  -1327504.5 ]\n",
      " [ -731092.06 -1365963.8 ]\n",
      " [ -673616.8  -1258578.8 ]\n",
      " [ -602834.75 -1126329.1 ]\n",
      " [ -659280.75 -1231792.8 ]\n",
      " [ -692719.94 -1294269.9 ]\n",
      " [ -614966.9  -1148996.8 ]\n",
      " [ -672236.6  -1255999.6 ]\n",
      " [ -623964.06 -1165807.6 ]\n",
      " [ -689694.25 -1288616.9 ]\n",
      " [ -690280.44 -1289712.8 ]\n",
      " [ -696461.3  -1301260.4 ]\n",
      " [ -692362.56 -1293602.9 ]\n",
      " [ -701312.6  -1310324.8 ]\n",
      " [ -668182.5  -1248425.  ]\n",
      " [ -634496.4  -1185485.6 ]\n",
      " [ -654538.75 -1222932.6 ]\n",
      " [ -675536.7  -1262164.2 ]\n",
      " [ -774503.9  -1447074.6 ]\n",
      " [ -708314.44 -1323406.9 ]\n",
      " [ -657493.5  -1228453.2 ]\n",
      " [ -656468.1  -1226538.1 ]\n",
      " [ -676465.   -1263899.6 ]\n",
      " [ -686061.1  -1281830.  ]\n",
      " [ -672763.5  -1256984.1 ]\n",
      " [ -715186.25 -1336245.5 ]\n",
      " [ -704864.2  -1316961.  ]\n",
      " [ -633126.1  -1182926.1 ]\n",
      " [ -704550.4  -1316374.6 ]\n",
      " [ -660842.06 -1234709.2 ]\n",
      " [ -667058.44 -1246324.6 ]\n",
      " [ -658136.4  -1229654.5 ]\n",
      " [ -620694.8  -1159699.9 ]\n",
      " [ -598406.6  -1118056.5 ]\n",
      " [ -672640.   -1256752.6 ]\n",
      " [ -612806.7  -1144961.1 ]\n",
      " [ -677373.06 -1265597.1 ]\n",
      " [ -704432.56 -1316154.  ]\n",
      " [ -578129.   -1080169.9 ]\n",
      " [ -694289.5  -1297203.  ]\n",
      " [ -743387.3  -1388936.1 ]\n",
      " [ -670425.75 -1252616.8 ]\n",
      " [ -719833.4  -1344928.6 ]\n",
      " [ -689799.7  -1288813.6 ]\n",
      " [ -681243.75 -1272828.1 ]\n",
      " [ -706532.8  -1320078.4 ]\n",
      " [ -720064.   -1345359.8 ]\n",
      " [ -700918.   -1309587.2 ]\n",
      " [ -680924.75 -1272231.8 ]\n",
      " [ -681557.2  -1273414.5 ]\n",
      " [ -735035.94 -1373333.9 ]\n",
      " [ -691321.7  -1291657.2 ]\n",
      " [ -709761.7  -1326111.8 ]\n",
      " [ -689347.56 -1287969.2 ]\n",
      " [ -654920.06 -1223646.  ]\n",
      " [ -677916.1  -1266612.2 ]\n",
      " [ -635307.94 -1187002.1 ]\n",
      " [ -692528.   -1293911.5 ]\n",
      " [ -658605.25 -1230529.9 ]\n",
      " [ -666195.3  -1244712.1 ]\n",
      " [ -667571.8  -1247283.2 ]\n",
      " [ -655069.   -1223923.4 ]\n",
      " [ -635514.56 -1187389.  ]\n",
      " [ -671733.75 -1255060.1 ]\n",
      " [ -728276.1  -1360703.2 ]\n",
      " [ -701093.2  -1309914.9 ]\n",
      " [ -708454.8  -1323669.5 ]\n",
      " [ -713577.06 -1333240.  ]\n",
      " [ -667994.8  -1248074.8 ]\n",
      " [ -638029.8  -1192088.  ]\n",
      " [ -675407.5  -1261923.2 ]\n",
      " [ -635015.2  -1186455.  ]\n",
      " [ -673270.44 -1257930.9 ]\n",
      " [ -728388.5  -1360913.5 ]\n",
      " [ -686406.6  -1282474.4 ]\n",
      " [ -686070.94 -1281848.4 ]\n",
      " [ -635604.75 -1187555.9 ]\n",
      " [ -673377.56 -1258132.  ]\n",
      " [ -669535.1  -1250952.6 ]\n",
      " [ -687139.1  -1283842.4 ]\n",
      " [ -706427.25 -1319881.1 ]\n",
      " [ -683523.94 -1277088.9 ]\n",
      " [ -705039.25 -1317288.6 ]\n",
      " [ -724700.2  -1354021.2 ]\n",
      " [ -669162.94 -1250256.6 ]\n",
      " [ -730281.5  -1364450.2 ]\n",
      " [ -675153.56 -1261449.8 ]\n",
      " [ -695514.   -1299491.2 ]\n",
      " [ -696586.25 -1301493.6 ]\n",
      " [ -733732.3  -1370897.4 ]\n",
      " [ -692407.94 -1293687.9 ]\n",
      " [ -737530.5  -1377993.9 ]\n",
      " [ -641340.8  -1198273.6 ]\n",
      " [ -647431.4  -1209653.5 ]\n",
      " [ -656191.1  -1226020.2 ]\n",
      " [ -654412.44 -1222696.  ]\n",
      " [ -602241.44 -1125221.4 ]\n",
      " [ -702934.4  -1313354.5 ]\n",
      " [ -717450.4  -1340476.2 ]\n",
      " [ -620728.06 -1159761.6 ]\n",
      " [ -633286.06 -1183225.  ]\n",
      " [ -704618.06 -1316501.1 ]\n",
      " [ -699327.44 -1306615.9 ]\n",
      " [ -618061.4  -1154779.6 ]\n",
      " [ -673023.25 -1257469.2 ]\n",
      " [ -653746.56 -1221453.4 ]\n",
      " [ -821675.44 -1535210.1 ]\n",
      " [ -641273.7  -1198149.4 ]\n",
      " [ -702141.6  -1311873.2 ]\n",
      " [ -703917.94 -1315192.2 ]\n",
      " [ -649534.4  -1213583.1 ]\n",
      " [ -661490.4  -1235921.2 ]\n",
      " [ -639308.1  -1194477.1 ]\n",
      " [ -676692.1  -1264323.8 ]\n",
      " [ -628847.2  -1174931.4 ]\n",
      " [ -663620.75 -1239901.4 ]\n",
      " [ -636574.3  -1189368.2 ]\n",
      " [ -609891.2  -1139513.8 ]\n",
      " [ -661623.7  -1236170.6 ]\n",
      " [ -659814.56 -1232789.4 ]\n",
      " [ -661329.94 -1235621.  ]\n",
      " [ -674518.9  -1260264.2 ]\n",
      " [ -652390.4  -1218919.  ]\n",
      " [ -680331.5  -1271123.5 ]\n",
      " [ -633848.4  -1184275.9 ]\n",
      " [ -577494.   -1078982.9 ]\n",
      " [ -695071.2  -1298663.8 ]\n",
      " [ -625939.7  -1169499.1 ]\n",
      " [ -716700.94 -1339076.1 ]\n",
      " [ -717600.7  -1340757.6 ]\n",
      " [ -662040.2  -1236949.4 ]\n",
      " [ -688803.4  -1286953.1 ]\n",
      " [ -688664.94 -1286694.5 ]\n",
      " [ -632975.1  -1182643.9 ]\n",
      " [ -709719.1  -1326031.5 ]\n",
      " [ -647873.06 -1210478.5 ]\n",
      " [ -636025.06 -1188342.6 ]\n",
      " [ -693016.94 -1294824.8 ]\n",
      " [ -639637.94 -1195092.1 ]\n",
      " [ -665953.56 -1244260.4 ]\n",
      " [ -622623.2  -1163302.6 ]\n",
      " [ -715216.4  -1336303.  ]\n",
      " [ -659539.44 -1232276.4 ]\n",
      " [ -665550.1  -1243507.2 ]\n",
      " [ -669196.9  -1250319.6 ]\n",
      " [ -674958.5  -1261085.  ]\n",
      " [ -642997.94 -1201370.1 ]\n",
      " [ -663852.44 -1240334.  ]\n",
      " [ -648240.8  -1211165.9 ]\n",
      " [ -751115.3  -1403375.9 ]\n",
      " [ -698366.4  -1304820.  ]\n",
      " [ -699878.   -1307644.1 ]\n",
      " [ -754849.06 -1410350.6 ]\n",
      " [ -673882.56 -1259074.8 ]\n",
      " [ -626680.   -1170882.2 ]\n",
      " [ -702436.5  -1312424.9 ]\n",
      " [ -682227.5  -1274666.6 ]\n",
      " [ -722268.   -1349478.1 ]\n",
      " [ -622142.4  -1162403.2 ]\n",
      " [ -696872.7  -1302029.  ]\n",
      " [ -602513.94 -1125730.6 ]\n",
      " [ -694571.3  -1297729.4 ]\n",
      " [ -738943.44 -1380634.2 ]\n",
      " [ -660584.8  -1234229.5 ]\n",
      " [ -774544.94 -1447151.1 ]\n",
      " [ -671137.94 -1253946.9 ]\n",
      " [ -709358.8  -1325359.2 ]\n",
      " [ -701542.7  -1310753.9 ]\n",
      " [ -736664.   -1376375.1 ]\n",
      " [ -685952.75 -1281626.4 ]\n",
      " [ -692510.5  -1293879.8 ]\n",
      " [ -651594.5  -1217432.  ]\n",
      " [ -712075.3  -1330433.5 ]\n",
      " [ -626516.3  -1170576.8 ]\n",
      " [ -715990.6  -1337748.8 ]\n",
      " [ -728931.9  -1361928.  ]\n",
      " [ -690378.   -1289895.2 ]\n",
      " [ -667329.5  -1246831.4 ]\n",
      " [ -716818.6  -1339296.5 ]\n",
      " [ -720809.6  -1346752.6 ]\n",
      " [ -720918.4  -1346955.  ]\n",
      " [ -674648.   -1260504.6 ]\n",
      " [ -657028.56 -1227585.1 ]\n",
      " [ -692382.4  -1293639.4 ]\n",
      " [ -678641.75 -1267965.6 ]\n",
      " [ -657679.94 -1228801.1 ]\n",
      " [ -785600.   -1467805.9 ]\n",
      " [ -666486.   -1245255.1 ]\n",
      " [ -688275.3  -1285965.6 ]\n",
      " [ -689900.44 -1289002.5 ]\n",
      " [ -642655.6  -1200731.  ]\n",
      " [ -744698.1  -1391385.2 ]\n",
      " [ -604207.5  -1128894.  ]\n",
      " [ -669684.4  -1251231.6 ]\n",
      " [ -602358.   -1125438.9 ]\n",
      " [ -731993.9  -1367650.6 ]\n",
      " [ -646531.8  -1207973.5 ]\n",
      " [ -675678.3  -1262429.9 ]\n",
      " [ -708379.3  -1323528.  ]\n",
      " [ -757797.75 -1415860.8 ]\n",
      " [ -672530.75 -1256549.8 ]\n",
      " [ -688109.7  -1285656.8 ]\n",
      " [ -723906.7  -1352539.2 ]\n",
      " [ -651042.3  -1216400.9 ]\n",
      " [ -761300.94 -1422406.6 ]\n",
      " [ -688128.94 -1285691.9 ]\n",
      " [ -694885.75 -1298317.  ]\n",
      " [ -666373.56 -1245045.4 ]\n",
      " [ -681426.25 -1273168.6 ]\n",
      " [ -643258.8  -1201857.6 ]\n",
      " [ -658553.3  -1230434.  ]\n",
      " [ -696842.4  -1301973.2 ]\n",
      " [ -666438.5  -1245165.9 ]\n",
      " [ -643664.3  -1202616.  ]\n",
      " [ -652255.06 -1218665.5 ]\n",
      " [ -685253.1  -1280319.  ]\n",
      " [ -686129.8  -1281957.8 ]\n",
      " [ -645673.25 -1206369.4 ]\n",
      " [ -656974.8  -1227484.6 ]\n",
      " [ -644052.5  -1203340.6 ]\n",
      " [ -625806.06 -1169249.4 ]\n",
      " [ -671237.4  -1254132.6 ]\n",
      " [ -629383.94 -1175933.6 ]\n",
      " [ -678322.44 -1267370.  ]\n",
      " [ -700183.1  -1308214.8 ]\n",
      " [ -699013.94 -1306030.2 ]\n",
      " [ -632294.2  -1181370.8 ]\n",
      " [ -659375.7  -1231970.5 ]\n",
      " [ -687399.   -1284328.  ]\n",
      " [ -652721.8  -1219537.6 ]\n",
      " [ -674163.2  -1259598.4 ]\n",
      " [ -649357.2  -1213251.9 ]\n",
      " [ -701141.75 -1310005.6 ]\n",
      " [ -650363.3  -1215131.8 ]\n",
      " [ -726124.   -1356681.5 ]\n",
      " [ -676846.75 -1264612.2 ]\n",
      " [ -678236.06 -1267209.6 ]\n",
      " [ -634336.8  -1185188.4 ]\n",
      " [ -676678.56 -1264298.2 ]\n",
      " [ -734128.3  -1371637.4 ]\n",
      " [ -641852.2  -1199230.2 ]\n",
      " [ -711267.9  -1328925.5 ]\n",
      " [ -654095.3  -1222104.2 ]\n",
      " [ -691343.7  -1291698.2 ]\n",
      " [ -676714.94 -1264367.1 ]\n",
      " [ -641560.4  -1198684.  ]\n",
      " [ -656558.   -1226705.9 ]\n",
      " [ -686016.4  -1281745.4 ]\n",
      " [ -695416.7  -1299309.8 ]\n",
      " [ -686851.8  -1283306.5 ]\n",
      " [ -651170.9  -1216640.1 ]\n",
      " [ -666577.1  -1245425.4 ]\n",
      " [ -720340.75 -1345876.6 ]\n",
      " [ -747163.75 -1395993.2 ]\n",
      " [ -656271.1  -1226169.9 ]\n",
      " [ -712279.6  -1330815.4 ]\n",
      " [ -642670.44 -1200758.2 ]\n",
      " [ -645626.2  -1206280.1 ]\n",
      " [ -712386.5  -1331015.  ]\n",
      " [ -664297.1  -1241165.5 ]\n",
      " [ -642669.25 -1200755.6 ]\n",
      " [ -595437.56 -1112508.8 ]\n",
      " [ -658519.2  -1230368.9 ]\n",
      " [ -685564.44 -1280901.4 ]\n",
      " [ -660226.   -1233559.1 ]\n",
      " [ -655276.9  -1224311.8 ]\n",
      " [ -671363.8  -1254368.1 ]\n",
      " [ -691246.06 -1291515.5 ]\n",
      " [ -721831.06 -1348660.  ]\n",
      " [ -698867.5  -1305756.6 ]\n",
      " [ -716503.94 -1338707.6 ]\n",
      " [ -675400.9  -1261911.9 ]\n",
      " [ -682563.7  -1275294.4 ]\n",
      " [ -706845.2  -1320663.  ]\n",
      " [ -698787.2  -1305606.2 ]\n",
      " [ -697387.44 -1302991.  ]\n",
      " [ -700719.75 -1309216.9 ]\n",
      " [ -667865.   -1247831.9 ]\n",
      " [ -680498.56 -1271436.5 ]\n",
      " [ -683278.9  -1276629.8 ]\n",
      " [ -655076.2  -1223936.2 ]\n",
      " [ -686325.56 -1282323.4 ]\n",
      " [ -653966.5  -1221864.6 ]\n",
      " [ -665792.3  -1243959.2 ]\n",
      " [ -682163.56 -1274546.5 ]\n",
      " [ -655310.06 -1224375.2 ]\n",
      " [ -671133.3  -1253937.8 ]\n",
      " [ -699666.4  -1307249.4 ]\n",
      " [ -711844.94 -1330002.4 ]\n",
      " [ -653940.3  -1221815.4 ]\n",
      " [ -649999.25 -1214451.9 ]\n",
      " [ -717069.4  -1339764.2 ]\n",
      " [ -672336.6  -1256186.5 ]\n",
      " [ -702373.2  -1312306.6 ]\n",
      " [ -719297.5  -1343926.9 ]\n",
      " [ -653005.3  -1220066.6 ]\n",
      " [ -720613.8  -1346386.6 ]\n",
      " [ -659753.   -1232675.1 ]\n",
      " [ -772196.25 -1442763.4 ]\n",
      " [ -700956.25 -1309658.8 ]\n",
      " [ -705464.4  -1318082.6 ]\n",
      " [ -670726.25 -1253177.5 ]\n",
      " [ -729506.44 -1363000.8 ]\n",
      " [ -683244.94 -1276567.  ]\n",
      " [ -639882.56 -1195549.6 ]\n",
      " [ -704485.94 -1316253.8 ]\n",
      " [ -677620.1  -1266058.  ]\n",
      " [ -660850.56 -1234726.5 ]\n",
      " [ -728176.4  -1360516.5 ]\n",
      " [ -681136.94 -1272629.4 ]\n",
      " [ -657371.7  -1228225.6 ]\n",
      " [ -720800.7  -1346735.9 ]\n",
      " [ -672915.   -1257267.5 ]\n",
      " [ -685072.6  -1279983.  ]\n",
      " [ -646095.8  -1207159.  ]\n",
      " [ -709659.3  -1325920.  ]\n",
      " [ -701756.25 -1311153.6 ]\n",
      " [ -639563.6  -1194953.4 ]\n",
      " [ -606139.6  -1132504.8 ]\n",
      " [ -698501.4  -1305071.9 ]\n",
      " [ -702749.3  -1313009.  ]\n",
      " [ -636836.8  -1189858.  ]\n",
      " [ -629793.1  -1176698.  ]\n",
      " [ -690732.7  -1290557.6 ]\n",
      " [ -662135.25 -1237126.9 ]\n",
      " [ -717714.56 -1340970.  ]\n",
      " [ -674549.56 -1260320.9 ]\n",
      " [ -746479.7  -1394714.4 ]\n",
      " [ -664160.5  -1240909.4 ]\n",
      " [ -655961.5  -1225591.1 ]\n",
      " [ -635121.9  -1186654.8 ]\n",
      " [ -685781.5  -1281306.2 ]\n",
      " [ -682817.   -1275767.6 ]\n",
      " [ -755549.8  -1411660.9 ]\n",
      " [ -693573.4  -1295864.8 ]\n",
      " [ -623522.3  -1164982.2 ]\n",
      " [ -713136.6  -1332416.8 ]\n",
      " [ -660798.1  -1234628.6 ]\n",
      " [ -713638.1  -1333353.  ]\n",
      " [ -657822.7  -1229069.4 ]\n",
      " [ -674002.25 -1259298.4 ]\n",
      " [ -736985.2  -1376974.6 ]\n",
      " [ -718954.5  -1343287.8 ]\n",
      " [ -712868.75 -1331915.5 ]]\n",
      "Final Label Distribution: [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.gc2 = nn.Linear(hidden_features, out_features)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = torch.relu(self.gc1(torch.matmul(adj, x)))  # Graph convolution layer 1\n",
    "        initial_predictions = x.clone().detach().numpy()  # Store initial predictions after GC\n",
    "        \n",
    "        x = self.gc2(torch.matmul(adj, x))  # Graph convolution layer 2\n",
    "        \n",
    "        cross_omics_discovery_tensor = torch.matmul(adj, x).detach().numpy()  # Derived from adjacency matrices\n",
    "        final_label_distribution = F.softmax(x, dim=1).detach().numpy()  # Class probabilities\n",
    "        \n",
    "        return initial_predictions, cross_omics_discovery_tensor, final_label_distribution\n",
    "\n",
    "# Example Usage\n",
    "num_nodes = 351\n",
    "in_features = 3\n",
    "hidden_features = 4\n",
    "out_features = 2\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.rand((num_nodes, in_features))  # Random node features\n",
    "adj = torch.randint(0, 2, (num_nodes, num_nodes)).float()  # Random adjacency matrix\n",
    "\n",
    "model = GCN(in_features, hidden_features, out_features)\n",
    "initial_preds, cross_omics_tensor, final_labels = model(x, adj)\n",
    "\n",
    "print(\"Initial Predictions:\", initial_preds)\n",
    "print(\"Cross-Omics Discovery Tensor:\", cross_omics_tensor)\n",
    "print(\"Final Label Distribution:\", final_labels)\n",
    "\n",
    "# Save outputs as CSV\n",
    "pd.DataFrame(initial_preds).to_csv(\"initial_predictions.csv\", index=False, header=False)\n",
    "pd.DataFrame(cross_omics_tensor).to_csv(\"cross_omics_discovery_tensor.csv\", index=False, header=False)\n",
    "pd.DataFrame(final_labels).to_csv(\"final_label_distribution.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "206+145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data_folder, view_list, num_class,\n",
    "               lr_e_pretrain, lr_e, lr_c, \n",
    "               num_epoch_pretrain, num_epoch):\n",
    "    test_inverval = 50\n",
    "    num_view = len(view_list)\n",
    "    dim_hvcdn = pow(num_class,num_view)\n",
    "    if data_folder == 'dataset_rosmap':\n",
    "        adj_parameter = 2\n",
    "        dim_he_list = [200,200,100]\n",
    "    #if data_folder == 'BRCA':\n",
    "        #adj_parameter = 10\n",
    "        #dim_he_list = [400,400,200]\n",
    "    data_tr_list, data_trte_list, trte_idx, labels_trte = prepare_trte_data(data_folder, view_list)\n",
    "    labels_tr_tensor = torch.LongTensor(labels_trte[trte_idx[\"tr\"]])\n",
    "    onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)\n",
    "    sample_weight_tr = cal_sample_weight(labels_trte[trte_idx[\"tr\"]], num_class)\n",
    "    sample_weight_tr = torch.FloatTensor(sample_weight_tr)\n",
    "    if cuda:\n",
    "        labels_tr_tensor = labels_tr_tensor.cuda()\n",
    "        onehot_labels_tr_tensor = onehot_labels_tr_tensor.cuda()\n",
    "        sample_weight_tr = sample_weight_tr.cuda()\n",
    "    adj_tr_list, adj_te_list = gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter)\n",
    "    dim_list = [x.shape[1] for x in data_tr_list]\n",
    "    model_dict = init_model_dict(num_view, num_class, dim_list, dim_he_list, dim_hvcdn)\n",
    "    for m in model_dict:\n",
    "        if cuda:\n",
    "            model_dict[m].cuda()\n",
    "    \n",
    "    print(\"\\nPretrain GCNs...\")\n",
    "    optim_dict = init_optim(num_view, model_dict, lr_e_pretrain, lr_c)\n",
    "    for epoch in range(num_epoch_pretrain):\n",
    "        train_epoch(data_tr_list, adj_tr_list, labels_tr_tensor, \n",
    "                    onehot_labels_tr_tensor, sample_weight_tr, model_dict, optim_dict, train_VCDN=False)\n",
    "    print(\"\\nTraining...\")\n",
    "    optim_dict = init_optim(num_view, model_dict, lr_e, lr_c)\n",
    "    for epoch in range(num_epoch+1):\n",
    "        train_epoch(data_tr_list, adj_tr_list, labels_tr_tensor, \n",
    "                    onehot_labels_tr_tensor, sample_weight_tr, model_dict, optim_dict)\n",
    "        if epoch % test_inverval == 0:\n",
    "            te_prob = test_epoch(data_trte_list, adj_te_list, trte_idx[\"te\"], model_dict)\n",
    "            print(\"\\nTest: Epoch {:d}\".format(epoch))\n",
    "            if num_class == 2:\n",
    "                print(\"Test ACC: {:.3f}\".format(accuracy_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))))\n",
    "                print(\"Test F1: {:.3f}\".format(f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))))\n",
    "                print(\"Test AUC: {:.3f}\".format(roc_auc_score(labels_trte[trte_idx[\"te\"]], te_prob[:,1])))\n",
    "            else:\n",
    "                print(\"Test ACC: {:.3f}\".format(accuracy_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1))))\n",
    "                print(\"Test F1 weighted: {:.3f}\".format(f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1), average='weighted')))\n",
    "                print(\"Test F1 macro: {:.3f}\".format(f1_score(labels_trte[trte_idx[\"te\"]], te_prob.argmax(1), average='macro')))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pretrain GCNs...\n",
      "\n",
      "Training...\n",
      "\n",
      "Test: Epoch 0\n",
      "Test ACC: 0.321\n",
      "Test F1: 0.265\n",
      "Test AUC: 0.205\n",
      "\n",
      "\n",
      "Test: Epoch 50\n",
      "Test ACC: 0.698\n",
      "Test F1: 0.765\n",
      "Test AUC: 0.809\n",
      "\n",
      "\n",
      "Test: Epoch 100\n",
      "Test ACC: 0.689\n",
      "Test F1: 0.759\n",
      "Test AUC: 0.859\n",
      "\n",
      "\n",
      "Test: Epoch 150\n",
      "Test ACC: 0.821\n",
      "Test F1: 0.840\n",
      "Test AUC: 0.883\n",
      "\n",
      "\n",
      "Test: Epoch 200\n",
      "Test ACC: 0.802\n",
      "Test F1: 0.826\n",
      "Test AUC: 0.877\n",
      "\n",
      "\n",
      "Test: Epoch 250\n",
      "Test ACC: 0.802\n",
      "Test F1: 0.817\n",
      "Test AUC: 0.890\n",
      "\n",
      "\n",
      "Test: Epoch 300\n",
      "Test ACC: 0.821\n",
      "Test F1: 0.840\n",
      "Test AUC: 0.889\n",
      "\n",
      "\n",
      "Test: Epoch 350\n",
      "Test ACC: 0.802\n",
      "Test F1: 0.814\n",
      "Test AUC: 0.890\n",
      "\n",
      "\n",
      "Test: Epoch 400\n",
      "Test ACC: 0.792\n",
      "Test F1: 0.810\n",
      "Test AUC: 0.885\n",
      "\n",
      "\n",
      "Test: Epoch 450\n",
      "Test ACC: 0.802\n",
      "Test F1: 0.817\n",
      "Test AUC: 0.887\n",
      "\n",
      "\n",
      "Test: Epoch 500\n",
      "Test ACC: 0.802\n",
      "Test F1: 0.821\n",
      "Test AUC: 0.884\n",
      "\n",
      "\n",
      "Test: Epoch 550\n",
      "Test ACC: 0.755\n",
      "Test F1: 0.768\n",
      "Test AUC: 0.870\n",
      "\n",
      "\n",
      "Test: Epoch 600\n",
      "Test ACC: 0.783\n",
      "Test F1: 0.796\n",
      "Test AUC: 0.884\n",
      "\n",
      "\n",
      "Test: Epoch 650\n",
      "Test ACC: 0.802\n",
      "Test F1: 0.800\n",
      "Test AUC: 0.886\n",
      "\n",
      "\n",
      "Test: Epoch 700\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.745\n",
      "Test AUC: 0.880\n",
      "\n",
      "\n",
      "Test: Epoch 750\n",
      "Test ACC: 0.802\n",
      "Test F1: 0.788\n",
      "Test AUC: 0.875\n",
      "\n",
      "\n",
      "Test: Epoch 800\n",
      "Test ACC: 0.764\n",
      "Test F1: 0.783\n",
      "Test AUC: 0.878\n",
      "\n",
      "\n",
      "Test: Epoch 850\n",
      "Test ACC: 0.792\n",
      "Test F1: 0.800\n",
      "Test AUC: 0.885\n",
      "\n",
      "\n",
      "Test: Epoch 900\n",
      "Test ACC: 0.764\n",
      "Test F1: 0.783\n",
      "Test AUC: 0.870\n",
      "\n",
      "\n",
      "Test: Epoch 950\n",
      "Test ACC: 0.792\n",
      "Test F1: 0.780\n",
      "Test AUC: 0.882\n",
      "\n",
      "\n",
      "Test: Epoch 1000\n",
      "Test ACC: 0.783\n",
      "Test F1: 0.777\n",
      "Test AUC: 0.859\n",
      "\n",
      "\n",
      "Test: Epoch 1050\n",
      "Test ACC: 0.783\n",
      "Test F1: 0.785\n",
      "Test AUC: 0.877\n",
      "\n",
      "\n",
      "Test: Epoch 1100\n",
      "Test ACC: 0.783\n",
      "Test F1: 0.796\n",
      "Test AUC: 0.873\n",
      "\n",
      "\n",
      "Test: Epoch 1150\n",
      "Test ACC: 0.783\n",
      "Test F1: 0.772\n",
      "Test AUC: 0.875\n",
      "\n",
      "\n",
      "Test: Epoch 1200\n",
      "Test ACC: 0.792\n",
      "Test F1: 0.784\n",
      "Test AUC: 0.873\n",
      "\n",
      "\n",
      "Test: Epoch 1250\n",
      "Test ACC: 0.726\n",
      "Test F1: 0.756\n",
      "Test AUC: 0.873\n",
      "\n",
      "\n",
      "Test: Epoch 1300\n",
      "Test ACC: 0.802\n",
      "Test F1: 0.800\n",
      "Test AUC: 0.879\n",
      "\n",
      "\n",
      "Test: Epoch 1350\n",
      "Test ACC: 0.745\n",
      "Test F1: 0.765\n",
      "Test AUC: 0.867\n",
      "\n",
      "\n",
      "Test: Epoch 1400\n",
      "Test ACC: 0.792\n",
      "Test F1: 0.784\n",
      "Test AUC: 0.866\n",
      "\n",
      "\n",
      "Test: Epoch 1450\n",
      "Test ACC: 0.764\n",
      "Test F1: 0.742\n",
      "Test AUC: 0.852\n",
      "\n",
      "\n",
      "Test: Epoch 1500\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.765\n",
      "Test AUC: 0.860\n",
      "\n",
      "\n",
      "Test: Epoch 1550\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.782\n",
      "Test AUC: 0.881\n",
      "\n",
      "\n",
      "Test: Epoch 1600\n",
      "Test ACC: 0.755\n",
      "Test F1: 0.772\n",
      "Test AUC: 0.862\n",
      "\n",
      "\n",
      "Test: Epoch 1650\n",
      "Test ACC: 0.764\n",
      "Test F1: 0.737\n",
      "Test AUC: 0.840\n",
      "\n",
      "\n",
      "Test: Epoch 1700\n",
      "Test ACC: 0.792\n",
      "Test F1: 0.800\n",
      "Test AUC: 0.870\n",
      "\n",
      "\n",
      "Test: Epoch 1750\n",
      "Test ACC: 0.783\n",
      "Test F1: 0.772\n",
      "Test AUC: 0.861\n",
      "\n",
      "\n",
      "Test: Epoch 1800\n",
      "Test ACC: 0.830\n",
      "Test F1: 0.820\n",
      "Test AUC: 0.871\n",
      "\n",
      "\n",
      "Test: Epoch 1850\n",
      "Test ACC: 0.783\n",
      "Test F1: 0.793\n",
      "Test AUC: 0.866\n",
      "\n",
      "\n",
      "Test: Epoch 1900\n",
      "Test ACC: 0.755\n",
      "Test F1: 0.772\n",
      "Test AUC: 0.862\n",
      "\n",
      "\n",
      "Test: Epoch 1950\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.750\n",
      "Test AUC: 0.868\n",
      "\n",
      "\n",
      "Test: Epoch 2000\n",
      "Test ACC: 0.745\n",
      "Test F1: 0.738\n",
      "Test AUC: 0.863\n",
      "\n",
      "\n",
      "Test: Epoch 2050\n",
      "Test ACC: 0.755\n",
      "Test F1: 0.755\n",
      "Test AUC: 0.860\n",
      "\n",
      "\n",
      "Test: Epoch 2100\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.765\n",
      "Test AUC: 0.852\n",
      "\n",
      "\n",
      "Test: Epoch 2150\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.782\n",
      "Test AUC: 0.872\n",
      "\n",
      "\n",
      "Test: Epoch 2200\n",
      "Test ACC: 0.764\n",
      "Test F1: 0.766\n",
      "Test AUC: 0.857\n",
      "\n",
      "\n",
      "Test: Epoch 2250\n",
      "Test ACC: 0.783\n",
      "Test F1: 0.768\n",
      "Test AUC: 0.865\n",
      "\n",
      "\n",
      "Test: Epoch 2300\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.789\n",
      "Test AUC: 0.868\n",
      "\n",
      "\n",
      "Test: Epoch 2350\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.793\n",
      "Test AUC: 0.880\n",
      "\n",
      "\n",
      "Test: Epoch 2400\n",
      "Test ACC: 0.764\n",
      "Test F1: 0.775\n",
      "Test AUC: 0.865\n",
      "\n",
      "\n",
      "Test: Epoch 2450\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.793\n",
      "Test AUC: 0.863\n",
      "\n",
      "\n",
      "Test: Epoch 2500\n",
      "Test ACC: 0.774\n",
      "Test F1: 0.778\n",
      "Test AUC: 0.886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    data_folder = 'dataset_rosmap'\n",
    "    view_list = [1,2,3]\n",
    "    num_epoch_pretrain = 500\n",
    "    num_epoch = 2500\n",
    "    lr_e_pretrain = 1e-3\n",
    "    lr_e = 5e-4\n",
    "    lr_c = 1e-3\n",
    "    \n",
    "    if data_folder == 'dataset_rosmap':\n",
    "        num_class = 2\n",
    "    #if data_folder == 'BRCA':\n",
    "        #num_class = 5\n",
    "    \n",
    "    train_test(data_folder, view_list, num_class,\n",
    "               lr_e_pretrain, lr_e, lr_c, \n",
    "               num_epoch_pretrain, num_epoch)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([351, 200]) torch.Size([352, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load node features from CSV (Assuming each row is a node's features)\n",
    "node_features_df = pd.read_csv(\"view_0_all_data.csv\", header=None)\n",
    "x = torch.tensor(node_features_df.values, dtype=torch.float32)\n",
    "\n",
    "# Load adjacency matrix from CSV (Assuming it's an NxN matrix)\n",
    "adj_df = pd.read_csv(\"tensor_view_3.csv\", header=None)\n",
    "adj = torch.tensor(adj_df.values, dtype=torch.float32)\n",
    "\n",
    "# Ensure x and adj are tensors with appropriate shapes\n",
    "print(x.shape, adj.shape)  # x should be (num_nodes, in_features), adj should be (num_nodes, num_nodes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
